---
title: AWS S3를 외부 호스팅 업체를 통해 사용 중 발생한 문제
date: 2025-09-25
categories: [AWS, cloud]
tags: [AWS, Runpod, cloud, S3]
description: Runpod의 Storage를 구매해놓고 8.8GB 파일을 올리지 못하여 가만히 돈만 내고 있던 상황을 해결합니다.
toc: false
comments: false
math: true
mermaid: false
---

# 문제 상황

Runpod이라는 AWS나 Google Cloud 같은 서비스보단 작지만 개인에게 적합한 클라우드 서비스를 이용하고 있었습니다. 그런데 오늘(2025/09/25) 아침에 작업을 이어가려고 켜니까 안켜져서 시스템 로그를 확인해보니 계속 도커 허브에서 이미지를 패치하고 있었습니다. 약 네 시간 정도 걸렸고 나중엔 들어갈 수 있게 돼서 들어갔더니 이번엔 남은 GPU가 없어서 CPU only로만 팟을 만들 수 있었습니다.  

그래서 데이터를 빼서 스토리지에 넣으면 스토리지를 기반으로 GPU가 있는 데이터센터를 골라 거기서 팟을 생성하고 스토리지를 연결하면 되겠다는 생각에 스토리지를 만들려고 했었습니다.  

그리고 즉시 스토리지를 생성했고, 스토리지는 AWS S3를 기반으로 하고 있었습니다. 그다음 팟에 있던 파일들을 SCP(Secure Copy Protocol)를 통해 로컬로 옮기고 이걸 다시 스토리지로 옮기려고 했으나, 한 파일이 8.8GB 정도의 크기여서, 당연히 일반 cp(aws s3 cp)로는 못 옮기고, multipart upload라는 것을 통해 업로드 해야 했습니다.  

문제는 이 multipart upload가 또 500MB 이상, 5GB 이하 파일만 한 번의 multipart upload 요청으로 보낼 수 있어서, 이걸로 보내려면 파일을 쪼개거나 다른 방법을 찾아야 했었습니다.

# 




scp -r -P [port] -i [identifile] [src] [dst]

https://stackoverflow.com/questions/10355941/how-can-i-copy-files-bigger-than-5-gb-in-amazon-s3
https://docs.aws.amazon.com/AmazonS3/latest/API/API_CompleteMultipartUpload.html
https://docs.aws.amazon.com/cli/latest/reference/s3api/complete-multipart-upload.html
https://docs.aws.amazon.com/cli/latest/reference/s3api/upload-part.html
https://docs.aws.amazon.com/cli/latest/reference/s3api/create-multipart-upload.html
https://github.com/runpod/runpod-s3-examples/blob/main/upload_large_file.py#L250
https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html
https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-commandstructure.html
https://repost.aws/knowledge-center/s3-access-denied-listobjects-sync
https://docs.runpod.io/pods/storage/transfer-files#transfer-with-scp
https://docs.runpod.io/serverless/storage/s3-api#multipart-upload-operations
https://docs.runpod.io/pods/storage/transfer-files#sync-with-cloud-storage

aws s3api complete-multipart-upload --bucket m8u5k7zv5c --key 'multipart/01' --upload-id '1758803871215394963_m8u5k7zv5c__multipart_01' --endpoint-url https://s3api-eu-ro-1.runpod.io --region eu-ro-1

aws s3 ls --region eu-ro-1 --endpoint-url https://s3api-eu-ro-1.runpod.io s3://m8u5k7zv5c/multipart/

python upload_large_file.py --file ../data/train.parquet --bucket m8u5k7zv5c --access_key user_32ubfbBk7uFaWnURly4FdZUyEOb --secret_key rps_XHH32RR7M0FGHVMXMKAVVPUA8K98YZGT6DO35JGC1auudw --endpoint https://s3api-eu-ro-1.runpod.io --region EU-RO-1 --key 'multipart/01'

aws s3api upload-part --bucket m8u5k7zv5c --key 'multipart/01' --part-number 1 --body data/train.parquet --upload-id '1758803871215394963_m8u5k7zv5c__multipart_01' --endpoint-url https://s3api-eu-ro-1.runpod.io --region eu-ro-1

pip install boto3

aws s3 sync --region eu-ro-1 --endpoint-url https://s3api-eu-ro-1.runpod.io /home/bocchi/workspace/toss/data/ s3://m8u5k7zv5c/main/data/

export AWS_RETRY_MODE=standard
export AWS_MAX_ATTEMPTS=10

aws configure list
aws s3 ls s3://m8u5k7zv5c --debug


# By Gemini

When encountering an "S3 content too large" error, it is important to understand the limits and recommended practices for uploading objects to Amazon S3.

### S3 Object Size Limits:
- Individual Amazon S3 objects can range from 0 bytes to a maximum of 5 TB.
- The largest object that can be uploaded in a single PUT operation is 5 GB. 
### Recommended Upload Methods for Large Objects:
- Multipart Upload: For objects larger than 100 MB, Amazon recommends using the multipart upload capability. This method breaks the object into smaller parts, uploads them in parallel, and then reassembles them on S3. This improves throughput and allows for easier recovery from network errors by only re-uploading failed parts.
- Multipart upload involves three main steps:
    - Initiate: Obtain an upload ID using the CreateMultipartUpload API call.
    - Upload Parts: Divide the object into parts, get a presigned URL for each part, and upload the parts in parallel using the UploadPart API call.
    - Complete: Call the CompleteMultipartUpload API call to combine the parts into the final object. 
### Troubleshooting "Content Too Large" Errors:
- Application or Library Limits: If you are encountering a "content too large" error with files smaller than 5 GB, the limit might be imposed by your application code, the S3 library you are using, or a web server configuration (e.g., client_max_body_size in Nginx).
- API Gateway Limits: If you are using API Gateway in your upload process, be aware of its configurable payload size limits. For larger files, consider using a pre-signed URL generated by a Lambda function to bypass API Gateway's limits and upload directly to S3. 
- AWS WAF Rules: AWS WAF rules can also limit request body sizes. By default, WAF only inspects the first 16 KB of a request body. 
- CORS Issues: If you are uploading from a web interface and encounter CORS errors with larger files, ensure your S3 bucket's CORS configuration is correctly set up to allow requests from your origin and for the necessary HTTP methods.